{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c840140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-2025-04-14\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") # Initialize the embedding model with Google Generative AI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "816a736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "pdf_path =os.path.join(os.getcwd(),\"data\",\"sample.pdf\")\n",
    "loader = PyPDFLoader(pdf_path) # Load the PDF file\n",
    "documents = loader.load() # Split the full PDF into pages\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150, length_function=len) #chunk_size and chunk_overlap are hyperparameters\n",
    "chunks = text_splitter.split_documents(documents) #Split the pages into smaller chunks\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model) #Create a vector store from the chunks using the embedding model\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10}) # Create a retriever from the vector store for similarity search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "605daae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "parser = StrOutputParser() # Create a string output parser to parse the LLM's response\n",
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "\n",
    "def get_formatted_doc(documents):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in documents]) # Format the retrieved documents into a single string\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"]) # Create a prompt template for the question-answering task\n",
    "rag_chain = {\"context\": retriever | get_formatted_doc, \"question\":RunnablePassthrough()}| prompt | llm | parser # Create a RAG chain that retrieves relevant documents, formats them, and generates an answer using the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b13b952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2 performed competitively against its peers on standard academic benchmarks. Across code generation (Human-Eval, MBPP), world knowledge (NaturalQuestions, TriviaQA), and other grouped academic tasks, Llama 2 consistently outperformed previous Llama 1 models and showed improved results over other open-source models like MosaicML Pretrained Transformer (MPT) and Falcon. Larger Llama 2 models (such as 70B) achieved higher benchmark scores than smaller models and were often on par with, or close to, the performance of ChatGPT in certain evaluations.\\n\\nOverall, the results indicate that Llama 2 represents a significant step forward compared to earlier open-source models, especially as model size increases, and can match or closely approach the performance of leading commercial models on various tasks. However, its proficiency in languages other than English remains limited.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"How did Llama 2 perform against it peers?\") # Invoke the RAG chain with a sample question"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
